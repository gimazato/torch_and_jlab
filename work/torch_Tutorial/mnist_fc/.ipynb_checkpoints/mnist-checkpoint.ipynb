{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load\n",
    "\n",
    "MNISTをロードする関数を作りましょう。\n",
    "\n",
    "PyTorchでは、TorchVisionというモジュールでデータセットを管理しています。\n",
    "\n",
    "PyTorchでは、データローダーという形でデータを取り扱うことが大きな特徴の一つです。\n",
    "\n",
    "このデータローダーには、バッチサイズごとにまとめられたデータとラベルがまとまっています。\n",
    "\n",
    "さらにデータは、 torch.tensor というテンソルの形で扱いますが、データローダーにおけるデータの形は（batch, channel, dimension）という順番になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    "\n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、 torch.utils.data.DataLoader() では、第一引数に「データセット」を取ります。\n",
    "\n",
    "今回は、その第一引数に datasets.MNIST() というMNISTのデータを扱うためのクラスインスタンスが与えられていることが分かります。\n",
    "\n",
    "このクラス（ datasets.MNIST()）では、コンストラクタとして第一引数にデータのダウンロード先を指定し、そのほかに訓練データか否か( train=True なら訓練データ、 train=False ならテストデータ)を指定したり、 transform= でデータを正規化したりできます。\n",
    "\n",
    "今回は、画素値の最大値を intensity 倍するような形ですが、他によく見る形として、\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5,), (0.5,))  # ここが違う\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    "        \n",
    "のように、平均と分散を指定すると良い精度になる場合もあります。\n",
    "\n",
    "今回用意した関数では、戻り値として各ローダーを辞書型変数にして返しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Network\n",
    "\n",
    "まずは、今回使うネットワークを定義していきます。\n",
    "\n",
    "PyTorchでは、 torch.nn.Module というクラスを継承して、オリジナルのネットワークを構築していきます。\n",
    "\n",
    "今回は MyNet という名前でネットワークを作っていきますが、ネットワーク構成はシンプルに「入力層(784) – 中間層(1000) – 出力層(10)」の3層構造とします。\n",
    "\n",
    "中間層の活性化関数に「シグモイド(sigmoid)関数」\n",
    "出力は確率にしたいので「ソフトマックス(softmax)関数」\n",
    "中間層の活性化関数に「シグモイド( sigmoid )関数」を、出力は確率にしたいので「ソフトマックス( softmax )関数」を使用します。\n",
    "\n",
    "今回は、MNISTという簡単なタスクで、なおかつ畳み込み層はないので、よく使用される ReLU関数 は使いません。(もちろん使っても良いです笑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return f.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main ここから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568916d639b843e7963eb7a0c33b2b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550b8cbde3f84c51829bef7b616943dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf47c9b1c8b4c56a8f813ca5819af4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8cfa24ba8041caacca4e038177cab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.319878101348877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.8018161058425903\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.4264224767684937\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.0580652952194214\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.954149603843689\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.8280723094940186\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.758462131023407\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.5285146832466125\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.48852384090423584\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.5442034602165222\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.41750568151474\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.40786442160606384\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.3118322789669037\n",
      "\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.46513739228248596\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.4095230996608734\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.41669878363609314\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.3233024477958679\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.457470566034317\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.26884201169013977\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.23983894288539886\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.3025517165660858\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.260715126991272\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.3918391764163971\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.31364941596984863\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.260357528924942\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.1966160237789154\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.40851566195487976\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.24860695004463196\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.3006417453289032\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.33950263261795044\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.3490258753299713\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.2446001172065735\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.47538599371910095\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.4189879298210144\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.32951658964157104\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.3782351315021515\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.2478303760290146\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.30804499983787537\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.3127793073654175\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.29016104340553284\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.23361365497112274\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.25227534770965576\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.35650184750556946\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.29778432846069336\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.30745962262153625\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.30009177327156067\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.23373770713806152\n",
      "Test loss (avg): 0.26041782155036924, Accuracy: 0.924\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.162120521068573\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.3017682135105133\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.19952842593193054\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.24272321164608002\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.2930692434310913\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.1836894154548645\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.2969832718372345\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.2857361137866974\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.2870109975337982\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.19173894822597504\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.17678430676460266\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.2898994982242584\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.33723506331443787\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.33486855030059814\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.23768429458141327\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.19681547582149506\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.29923000931739807\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.29696211218833923\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.2162410169839859\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.30367547273635864\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.14310744404792786\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.3046450614929199\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.20551353693008423\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.2354137897491455\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.1765248030424118\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.3523652255535126\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.12465507537126541\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.19596266746520996\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.19058486819267273\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.2563839256763458\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.20564422011375427\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.17484384775161743\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.1953420341014862\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.15181311964988708\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.17096374928951263\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.22053970396518707\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.2721398174762726\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.2905183732509613\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.19406144320964813\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.22382619976997375\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.24911026656627655\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.19236207008361816\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.23740428686141968\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.30634090304374695\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.3073582649230957\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.22152820229530334\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.23484912514686584\n",
      "Test loss (avg): 0.193343453001976, Accuracy: 0.9423\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.091771699488163\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.10999450087547302\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.1841171234846115\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.15888279676437378\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.17656168341636658\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.15173035860061646\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.1681145578622818\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.30427661538124084\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.10949966311454773\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.197041854262352\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.12179552763700485\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.28817370533943176\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.2057466208934784\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.22646069526672363\n",
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.31648001074790955\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.1651940643787384\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.1411040872335434\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.2532965838909149\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.19927586615085602\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.24699188768863678\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.21108220517635345\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.20878306031227112\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.16265907883644104\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.14084571599960327\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.18728823959827423\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.13186237215995789\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.17154541611671448\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.18035490810871124\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.14816103875637054\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.11410452425479889\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.1850517988204956\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.18914836645126343\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.08243809640407562\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.17780202627182007\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.14451998472213745\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.1506209820508957\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.20237745344638824\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.12996193766593933\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.22529862821102142\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.16101378202438354\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.20918239653110504\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.15356680750846863\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.17787210643291473\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.21631158888339996\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.18325060606002808\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.13412968814373016\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.2256474792957306\n",
      "Test loss (avg): 0.1628232810497284, Accuracy: 0.9523\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.1410127878189087\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.21214193105697632\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.1816694140434265\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.08885004371404648\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.13710850477218628\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.14899493753910065\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.16896255314350128\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.25684529542922974\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.12951965630054474\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.13258369266986847\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.07496082782745361\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.1550901234149933\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.11634620279073715\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.07330568879842758\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.09969304502010345\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.17315691709518433\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.12287335842847824\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.1357313096523285\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.12593895196914673\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.1466364860534668\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.14655731618404388\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.15136325359344482\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.10729160904884338\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.18882790207862854\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.1098807156085968\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.11650127172470093\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.1795043796300888\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.15114445984363556\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.12784646451473236\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.13770289719104767\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.06817221641540527\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.23085415363311768\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.17853547632694244\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.20749500393867493\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.04323301836848259\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.1387804001569748\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.14044177532196045\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.12896056473255157\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.09970337152481079\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.16370975971221924\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.06370624154806137\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.11647096276283264\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.10750680416822433\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.18295468389987946\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.2163146436214447\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.12189127504825592\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.06854672729969025\n",
      "Test loss (avg): 0.12684606988132, Accuracy: 0.9603\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.0652339980006218\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.04751202464103699\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.0722900778055191\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.12933269143104553\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.18726928532123566\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.1191096380352974\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.08170664310455322\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.07412628084421158\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.23921525478363037\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.10579336434602737\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.10955332964658737\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.13749834895133972\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.0940675437450409\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.0923803523182869\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.0994383692741394\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.06778549402952194\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.03276355564594269\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.19218756258487701\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.11082865297794342\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.07067905366420746\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.1609337478876114\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.06489790230989456\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.1807345747947693\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.060731787234544754\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.1283322423696518\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.16196468472480774\n",
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.10858067125082016\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.105768583714962\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.08929800987243652\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.1356886327266693\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.15842437744140625\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.07816813141107559\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.1098494604229927\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.09970679879188538\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.09134357422590256\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.04580197483301163\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.07890866696834564\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.15875770151615143\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.07945159822702408\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.06482373178005219\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.13907501101493835\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.09967859089374542\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.12676800787448883\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.12907686829566956\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.12121972441673279\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.1534334421157837\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.1514294147491455\n",
      "Test loss (avg): 0.10555239989757538, Accuracy: 0.9686\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.07164767384529114\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.08874157816171646\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.10826684534549713\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.11381901055574417\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.13155242800712585\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.12828712165355682\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.15682557225227356\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.10898550599813461\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.06469015777111053\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.11566933989524841\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.06021098047494888\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.10037536919116974\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.053074393421411514\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.09624634683132172\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.032788947224617004\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.17583982646465302\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.08919575065374374\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.1129220649600029\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.08546754717826843\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.1037508174777031\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.13783502578735352\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.08970175683498383\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.10626930743455887\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.07616059482097626\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.0506875105202198\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.06372316181659698\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.09020240604877472\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.05622328817844391\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.1023474782705307\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.07443156838417053\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.05521542951464653\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.06627006083726883\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.03284972533583641\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.0808631181716919\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.04579053074121475\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.05830098316073418\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.06316627562046051\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.09938938915729523\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.03184587508440018\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.1157890185713768\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.08897175639867783\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.06538091599941254\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.04189002886414528\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.037493158131837845\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.06744170933961868\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.09716226905584335\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.10245271027088165\n",
      "Test loss (avg): 0.09401378881335258, Accuracy: 0.9719\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.08820708841085434\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.03669589012861252\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.046983394771814346\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.039863597601652145\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.06054886430501938\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.11126656830310822\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.1022072434425354\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.045363929122686386\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.15761715173721313\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.07175594568252563\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.06755862385034561\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.04968646541237831\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.07381585985422134\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.0622757151722908\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.08978711813688278\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.04631354659795761\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.13790737092494965\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.05112754926085472\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.11830700188875198\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.06986815482378006\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.03136087581515312\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.043196022510528564\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.06557001918554306\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.09041845053434372\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.05308208614587784\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.10096427798271179\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.03444822132587433\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.09287130832672119\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.033938243985176086\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.07355546951293945\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.05190542712807655\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.06483965367078781\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.052988383919000626\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.08217582106590271\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.09212997555732727\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.14295414090156555\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.0957212746143341\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.10127576440572739\n",
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.10853003710508347\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.06404628604650497\n",
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.047246962785720825\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.11575643718242645\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.09308737516403198\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.0905470997095108\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.02290286310017109\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.07106474041938782\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.09359565377235413\n",
      "Test loss (avg): 0.0835898035645485, Accuracy: 0.9758\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.04109421744942665\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.0652574896812439\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.03410077840089798\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.036249201744794846\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.06405413895845413\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.068306103348732\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.05276281386613846\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.09082760661840439\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.055045388638973236\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.04920785501599312\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.09873900562524796\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.07599932700395584\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.06914579123258591\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.034611187875270844\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.08926182985305786\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.11116494983434677\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.04652491584420204\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.04737135022878647\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.030509501695632935\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.029834343120455742\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.06451413780450821\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.07817939668893814\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.09177139401435852\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.09128914773464203\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.10276007652282715\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.02309742383658886\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.10706894099712372\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.06076600030064583\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.028057808056473732\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.04082270339131355\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.09991367161273956\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.06118672713637352\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.05993528664112091\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.11436983942985535\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.09290792047977448\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.03109477460384369\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.08477023243904114\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.058227088302373886\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.04282955825328827\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.057864654809236526\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.07316865772008896\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.08598334342241287\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.057546406984329224\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.072980597615242\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.12866650521755219\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.09363574534654617\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.07471408694982529\n",
      "Test loss (avg): 0.0843161232046783, Accuracy: 0.9733\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.056589480489492416\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.043062709271907806\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.03391120582818985\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.023524748161435127\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.06297249346971512\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.06854639947414398\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.010491822846233845\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.030603842809796333\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.06098943576216698\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.026520978659391403\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.050609052181243896\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.04327620938420296\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.10838298499584198\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.09282268583774567\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.09284670650959015\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.039529167115688324\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.03752191737294197\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.05807691439986229\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.049002304673194885\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.061474189162254333\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.03208480030298233\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.0801142007112503\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.03890392184257507\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.09482812136411667\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.019282866269350052\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.039114829152822495\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.06713270395994186\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.0879904106259346\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.1114218458533287\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.06723472476005554\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.08835313469171524\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.07586709409952164\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.060114163905382156\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.07110793143510818\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.03752095252275467\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.07608936727046967\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.06349045783281326\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.08668872714042664\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.08318991959095001\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.0210499819368124\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.03763538599014282\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.03087298944592476\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.04577655717730522\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.05913548916578293\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.0377931073307991\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.028446681797504425\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.019749239087104797\n",
      "Test loss (avg): 0.08070923969745636, Accuracy: 0.9753\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.03112807124853134\n",
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.040757421404123306\n",
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.02659575268626213\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.024401124566793442\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.03209532052278519\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.03284743055701256\n",
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.03201242536306381\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.022543825209140778\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.06543576717376709\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.01909860223531723\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.02748986892402172\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.04608408361673355\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.05177559703588486\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.06758327037096024\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.02765248343348503\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.02665005996823311\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.05882256105542183\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.02789202146232128\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.03801300749182701\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.027787525206804276\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.053778693079948425\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.03370077535510063\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.04175041243433952\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.030478352680802345\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.0376516692340374\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.04827575385570526\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.04252248257398605\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.05359511449933052\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.055038291960954666\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.03217140585184097\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.06651531904935837\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.04016609862446785\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.04756087437272072\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.05882399156689644\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.014516733586788177\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.06647353619337082\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.05463137477636337\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.06943072378635406\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.011238357983529568\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.013649294152855873\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.03682924062013626\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.07820144295692444\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.05506202578544617\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.028165413066744804\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.046219129115343094\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.016164660453796387\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.07710935175418854\n",
      "Test loss (avg): 0.07420865437239409, Accuracy: 0.9771\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.040376514196395874\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.035865798592567444\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.019723286852240562\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.01830609142780304\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.033348530530929565\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.029297951608896255\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.03153158351778984\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.013545524328947067\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.029195701703429222\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.02883794717490673\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.009797964245080948\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.04828450083732605\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.021547816693782806\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.010356090031564236\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.04006195068359375\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.020642444491386414\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.04044694826006889\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.050977256149053574\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.040514588356018066\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.027921821922063828\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.017156006768345833\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.025948327034711838\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.06532146036624908\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.1314321607351303\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.024142460897564888\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.01701069250702858\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.04368792101740837\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.03293246775865555\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.04892898350954056\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.04443815350532532\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.04526848345994949\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.031176891177892685\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.03415656089782715\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.0323452427983284\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.01412786915898323\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.02816755883395672\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.027228225022554398\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.07582754641771317\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.04566938057541847\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.018162619322538376\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.02954990603029728\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.038852494210004807\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.064720518887043\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.03301391378045082\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.017310209572315216\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.02866814099252224\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.07496989518404007\n",
      "Test loss (avg): 0.06715195031166077, Accuracy: 0.9794\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.03021521493792534\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.025858931243419647\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.01952982507646084\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.024628086015582085\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.01851716823875904\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.02814369462430477\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.010450389236211777\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.020127391442656517\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.014691021293401718\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.012997615151107311\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.013319691643118858\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.02518676593899727\n",
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.029161404818296432\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.01479825284332037\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.06280545890331268\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.022390756756067276\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.0170290544629097\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.10724957287311554\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.014424639753997326\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.023368937894701958\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.02705315686762333\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.02476012520492077\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.04664110019803047\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.015058523043990135\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.052231624722480774\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.024357357993721962\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.014510521665215492\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.0174760352820158\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.019496895372867584\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.010163013823330402\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.017976369708776474\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.0248105488717556\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.02026071771979332\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.05362405255436897\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.021219192072749138\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.02140066772699356\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.04212091118097305\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.03542031720280647\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.015643468126654625\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.013301185332238674\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.05302395299077034\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.024409927427768707\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.07109993696212769\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.012986608780920506\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.03934961184859276\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.024069009348750114\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.03964490815997124\n",
      "Test loss (avg): 0.06953617745637894, Accuracy: 0.9778\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.007921496406197548\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.013070931658148766\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.028116580098867416\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.02257521264255047\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.037260089069604874\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.02129894122481346\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.00863155908882618\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.009456336498260498\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.024760782718658447\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.009411613456904888\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.008945847861468792\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.007605249527841806\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.01798810251057148\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.0236129779368639\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.032477594912052155\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.04561557248234749\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.03133579343557358\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.020589018240571022\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.007150935474783182\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.011026622727513313\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.01835854910314083\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.01624419167637825\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.015794064849615097\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.052200671285390854\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.02773475833237171\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.05101974308490753\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.021849749609827995\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.03992060571908951\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.010194134898483753\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.012518798001110554\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.018448365852236748\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.03279576823115349\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.017366426065564156\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.01465060468763113\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.010513047687709332\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.022880693897604942\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.028376543894410133\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.07209304720163345\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.04792081192135811\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.03616537153720856\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.057795438915491104\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.017104439437389374\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.020002534613013268\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.04510902985930443\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.02195628732442856\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.024724425747990608\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.003208784619346261\n",
      "Test loss (avg): 0.06970387752056122, Accuracy: 0.9783\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.014306171797215939\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.0205355454236269\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.008571462705731392\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.010070694610476494\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.010239635594189167\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.0207239780575037\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.018423525616526604\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.02214074321091175\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.032258592545986176\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.013720540329813957\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.01722589135169983\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.017513137310743332\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.016376890242099762\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.013120451010763645\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.020198777318000793\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.028030050918459892\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.01692519150674343\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.01299343816936016\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.012376382946968079\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.025074752047657967\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.021459661424160004\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.01693296805024147\n",
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.01332810241729021\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.010560558177530766\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.019316017627716064\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.014621914364397526\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.008989034220576286\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.018077459186315536\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.006766251288354397\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.014916019514203072\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.012715529650449753\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.016704032197594643\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.032210517674684525\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.010853360407054424\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.013583080843091011\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.00462016835808754\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.013902897946536541\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.007781856693327427\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.013347378931939602\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.02685071900486946\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.005488150287419558\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.015426619909703732\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.014448781497776508\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.025465326383709908\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.017721660435199738\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.006445471663028002\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.043332282453775406\n",
      "Test loss (avg): 0.06059284912315197, Accuracy: 0.9805\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.011201616376638412\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.009511558338999748\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.005115259438753128\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.022928014397621155\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.040338486433029175\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.01844668947160244\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.007652785629034042\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.016121363267302513\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.027524953708052635\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.0048525664024055\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.013107082806527615\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.005489717237651348\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.010525870136916637\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.011338305659592152\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.00509329279884696\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.00616834033280611\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.05275275185704231\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.023379001766443253\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.014827868901193142\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.017412465065717697\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.026592900976538658\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.016604401171207428\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.007432184647768736\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.005502813961356878\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.018962418660521507\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.02543928287923336\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.011092478409409523\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.009546819142997265\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.014479904435575008\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.015374446287751198\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.012401355430483818\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.013192982412874699\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.009024868719279766\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.007240614388138056\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.005526644643396139\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.011585424654185772\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.011663583107292652\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.021884514018893242\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.012361394241452217\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.015765663236379623\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.02612440288066864\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.009730232879519463\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.00785826612263918\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.02549857087433338\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.008625698275864124\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.011899609118700027\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.008496900089085102\n",
      "Test loss (avg): 0.06546202514767646, Accuracy: 0.9804\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.013649790547788143\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.009026482701301575\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.012518737465143204\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.013245395384728909\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.04004659876227379\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.003951852675527334\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.010995596647262573\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.020516999065876007\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.006129606626927853\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.02220148965716362\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.012405003421008587\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.005959771573543549\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.006448563653975725\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.03627745807170868\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.006489376537501812\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.0067720673978328705\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.03466139733791351\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.03969956189393997\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.004870343953371048\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.010895908810198307\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.007683248724788427\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.006011765915900469\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.01056666299700737\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.003748892340809107\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.009409767575562\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.01092685479670763\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.00607755221426487\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.02614729106426239\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.016141744330525398\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.01449665892869234\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.006329210940748453\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.007942157797515392\n",
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.018104443326592445\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.029368678107857704\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.021629273891448975\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.029576903209090233\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.007912172935903072\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.014650434255599976\n",
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.011023535393178463\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.012009591795504093\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.009529225528240204\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.02215907908976078\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.022053562104701996\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.009852137416601181\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.029952147975564003\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.034769825637340546\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.017942752689123154\n",
      "Test loss (avg): 0.06255376259759068, Accuracy: 0.9808\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.003570897737517953\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.013383273035287857\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.005637782160192728\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.00885104387998581\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.004737286828458309\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.003989000804722309\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.003015962429344654\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.02347266860306263\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.004400778096169233\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.019239716231822968\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.003712546778842807\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.00665064575150609\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.007935159839689732\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.007340377662330866\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.00866573117673397\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.021651741117239\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.010100319050252438\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.00713221775367856\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.006611648481339216\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.0065727452747523785\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.019784901291131973\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.005230694077908993\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.016296884045004845\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.004902318120002747\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.009855633601546288\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.00974248256534338\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.005111224949359894\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.008350866846740246\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.00437432574108243\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 0.014039590023458004\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.011686530895531178\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.01710743084549904\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.009949841536581516\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.008176442235708237\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.01064343936741352\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.0028812349773943424\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.009177641943097115\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.0021691257134079933\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.005784500390291214\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.00439794547855854\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.0043992288410663605\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.008889401331543922\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.007353628985583782\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.0055308216251432896\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.006976359523832798\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.00562015175819397\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.007506736554205418\n",
      "Test loss (avg): 0.06380103054642677, Accuracy: 0.9808\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.005302912089973688\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.005090526770800352\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.007961376570165157\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.0029499907977879047\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.0033744010142982006\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.00635970663279295\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.0034589143469929695\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.006054454017430544\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.003153918543830514\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.0045354790054261684\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.003755997633561492\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.002840508008375764\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.0031431277748197317\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.0030933136586099863\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.009599801152944565\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.005437423475086689\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.0029921503737568855\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.0071028536185622215\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.004929817281663418\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.012294433079659939\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.0026346147060394287\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.005828008521348238\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.005993937607854605\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.0046405671164393425\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.012048948556184769\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.005845408886671066\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.00986813846975565\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.012593246065080166\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.01033482700586319\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.00231204298324883\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.007059936877340078\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.009622635319828987\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.011150998063385487\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.003392288228496909\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.005326372571289539\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.004925443325191736\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.011203682981431484\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.005105014890432358\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 0.020523156970739365\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.003026248188689351\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.0041905734688043594\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.005870477762073278\n",
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.008829786442220211\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.004834272433072329\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.005542371887713671\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.009500951506197453\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.008337510749697685\n",
      "Test loss (avg): 0.06092147286236286, Accuracy: 0.9822\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.004834807012230158\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.0025375664699822664\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.007651881314814091\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.003224022453650832\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.005453785881400108\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.009276874363422394\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.004235899541527033\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.002540934132412076\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.004445988684892654\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.004745307378470898\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.004011559300124645\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.00301876338198781\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.008310731500387192\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.006253937259316444\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.0025801919400691986\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.004874584265053272\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.005803823471069336\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.005199097096920013\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.005025316495448351\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.01342709269374609\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.003968625329434872\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.015125516802072525\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.007891579531133175\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.002030604751780629\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.0032722854521125555\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.0016710530035197735\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.0029246476478874683\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.007703172974288464\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.011732645332813263\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.010328157804906368\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.007304918020963669\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.003435941645875573\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.0031740518752485514\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.004771357402205467\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.004017746541649103\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.004843234550207853\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.006531268823891878\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.013023349456489086\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.0046735224314033985\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.01566372439265251\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.0026730005629360676\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.005586840212345123\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.006033441051840782\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.0077806743793189526\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.004496952518820763\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.0030498835258185863\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.003983058035373688\n",
      "Test loss (avg): 0.06272736451886594, Accuracy: 0.9821\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.0048776003532111645\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.0034704350400716066\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.0067576137371361256\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.007274107541888952\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.0024383068084716797\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.0033083229791373014\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.002137420466169715\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.006224934943020344\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.00772725697606802\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.00518994964659214\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.0032879970967769623\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.0032739811576902866\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.0016111962031573057\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.004511796869337559\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.00958265084773302\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.0030543236061930656\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.003977672196924686\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.00231038685888052\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.007104751653969288\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.004506605211645365\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.01121191680431366\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.0023757172748446465\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.006938025821000338\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.005150525365024805\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.0029547784943133593\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.002807371784001589\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.004945649765431881\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.0031035877764225006\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.01083768717944622\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.0016166988061740994\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.0038361388724297285\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.0034422632306814194\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.003460778621956706\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.006350810639560223\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.0024641058407723904\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.002872122684493661\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.003377672750502825\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.0030155209824442863\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.003738227766007185\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.00215747463516891\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.004814170766621828\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.003676060354337096\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.0021790952887386084\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.010339013300836086\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.0073662418872118\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.0012256669579073787\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.004872225224971771\n",
      "Test loss (avg): 0.06625564698725939, Accuracy: 0.9811\n",
      "{'train_loss': [tensor(0.2499, grad_fn=<NllLossBackward>), tensor(0.1149, grad_fn=<NllLossBackward>), tensor(0.1120, grad_fn=<NllLossBackward>), tensor(0.1270, grad_fn=<NllLossBackward>), tensor(0.2245, grad_fn=<NllLossBackward>), tensor(0.1155, grad_fn=<NllLossBackward>), tensor(0.1280, grad_fn=<NllLossBackward>), tensor(0.1950, grad_fn=<NllLossBackward>), tensor(0.0819, grad_fn=<NllLossBackward>), tensor(0.0439, grad_fn=<NllLossBackward>), tensor(0.0699, grad_fn=<NllLossBackward>), tensor(0.0280, grad_fn=<NllLossBackward>), tensor(0.0113, grad_fn=<NllLossBackward>), tensor(0.0177, grad_fn=<NllLossBackward>), tensor(0.0164, grad_fn=<NllLossBackward>), tensor(0.0049, grad_fn=<NllLossBackward>), tensor(0.0039, grad_fn=<NllLossBackward>), tensor(0.0038, grad_fn=<NllLossBackward>), tensor(0.0078, grad_fn=<NllLossBackward>), tensor(0.0069, grad_fn=<NllLossBackward>)], 'test_loss': [0.26041782155036924, 0.193343453001976, 0.1628232810497284, 0.12684606988132, 0.10555239989757538, 0.09401378881335258, 0.0835898035645485, 0.0843161232046783, 0.08070923969745636, 0.07420865437239409, 0.06715195031166077, 0.06953617745637894, 0.06970387752056122, 0.06059284912315197, 0.06546202514767646, 0.06255376259759068, 0.06380103054642677, 0.06092147286236286, 0.06272736451886594, 0.06625564698725939], 'test_acc': [0.924, 0.9423, 0.9523, 0.9603, 0.9686, 0.9719, 0.9758, 0.9733, 0.9753, 0.9771, 0.9794, 0.9778, 0.9783, 0.9805, 0.9804, 0.9808, 0.9808, 0.9822, 0.9821, 0.9811]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # 学習回数\n",
    "    epoch = 20\n",
    "\n",
    "    # 学習結果の保存用\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    "\n",
    "    # ネットワークを構築\n",
    "    net: torch.nn.Module = MyNet()\n",
    "\n",
    "    # MNISTのデータローダーを取得\n",
    "    loaders = load_MNIST()\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        \"\"\" Training Part\"\"\"\n",
    "        loss = None\n",
    "        # 学習開始 (再開)\n",
    "        net.train(True)  # 引数は省略可能\n",
    "        for i, (data, target) in enumerate(loaders['train']):\n",
    "            # 全結合のみのネットワークでは入力を1次元に\n",
    "            # print(data.shape)  # torch.Size([128, 1, 28, 28])\n",
    "            data = data.view(-1, 28*28)\n",
    "            # print(data.shape)  # torch.Size([128, 784])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = f.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(e+1,\n",
    "                                                                                         (i+1)*128,\n",
    "                                                                                         loss.item())\n",
    "                      )\n",
    "\n",
    "        history['train_loss'].append(loss)\n",
    "\n",
    "        \"\"\" Test Part \"\"\"\n",
    "        # 学習のストップ\n",
    "        net.eval()  # または net.train(False) でも良い\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in loaders['test']:\n",
    "                data = data.view(-1, 28 * 28)\n",
    "                output = net(data)\n",
    "                test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= 10000\n",
    "\n",
    "        print('Test loss (avg): {}, Accuracy: {}'.format(test_loss,\n",
    "                                                         correct / 10000))\n",
    "\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(correct / 10000)\n",
    "\n",
    "    # 結果の出力と描画\n",
    "    print(history)\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
    "    plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epoch+1), history['test_acc'])\n",
    "    plt.title('test accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig('test_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
