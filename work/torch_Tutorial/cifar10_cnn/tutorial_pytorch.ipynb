{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Torch\n",
    "\n",
    "[ライトニングpytorch入門](https://qiita.com/sh-tatsuno/items/42fccff90c98103dffc9)\n",
    "\n",
    "[PyTorch入門 [公式Tutorial:DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZを読む]](https://qiita.com/north_redwing/items/30f9619f0ee727875250) -> 基本こっち主軸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #基本モジュール\n",
    "from torch.autograd import Variable #自動微分用\n",
    "import torch.nn as nn #ネットワーク構築用\n",
    "import torch.optim as optim #最適化関数\n",
    "import torch.nn.functional as F #ネットワーク用の様々な関数\n",
    "import torch.utils.data #データセット読み込み関連\n",
    "import torchvision #画像関連\n",
    "from torchvision import datasets, models, transforms #画像用データセット諸々"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "- pytorchはTensorという型で演算を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7417e+13,  3.0691e-41, -2.8707e+12],\n",
      "        [ 3.0691e-41, -3.4055e+12,  3.0691e-41],\n",
      "        [-2.4500e+13,  3.0691e-41, -9.7131e-21],\n",
      "        [ 0.0000e+00,  1.5835e-43,  0.0000e+00],\n",
      "        [-2.7325e+13,  3.0691e-41, -2.4585e+13]])\n",
      "tensor([[0.2979, 0.1188, 0.6558],\n",
      "        [0.9975, 0.4709, 0.4775],\n",
      "        [0.1576, 0.4557, 0.8044],\n",
      "        [0.0100, 0.1896, 0.5190],\n",
      "        [0.5217, 0.7507, 0.9679]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor(5,3)# Construct a tensor directly from data:\n",
    "print(x)\n",
    "\n",
    "y=torch.rand(5,3)\n",
    "print(y)\n",
    "\n",
    "z = torch.zeros(5,3, dtype=torch.long)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "- pytorchを利用するには全ての変数はTensorに変換する必要がある\n",
    "\n",
    " - Tensor -> numpy ：(Tensorの変数).numpy()\n",
    "\n",
    " - numpy -> Tensor :torch.from_numpy(Numpyの変数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24647075 0.12197757 0.9411779 ]\n",
      " [0.3448102  0.51629155 0.97023378]\n",
      " [0.76787071 0.04857577 0.31369453]\n",
      " [0.45920206 0.9655576  0.05411122]\n",
      " [0.89728857 0.05418069 0.625699  ]]\n",
      "tensor([[0.2465, 0.1220, 0.9412],\n",
      "        [0.3448, 0.5163, 0.9702],\n",
      "        [0.7679, 0.0486, 0.3137],\n",
      "        [0.4592, 0.9656, 0.0541],\n",
      "        [0.8973, 0.0542, 0.6257]], dtype=torch.float64)\n",
      "[[0.24647075 0.12197757 0.9411779 ]\n",
      " [0.3448102  0.51629155 0.97023378]\n",
      " [0.76787071 0.04857577 0.31369453]\n",
      " [0.45920206 0.9655576  0.05411122]\n",
      " [0.89728857 0.05418069 0.625699  ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(5, 3)\n",
    "y = torch.from_numpy(x)\n",
    "z = y.numpy()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参照渡し注意\n",
    "- ここで後半に注目しましょう. aから作成したbは, aを変更すると一緒に変更されいます. 参照渡しということです.\n",
    "- これ結構大事なことです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a= torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    "- Tensor型にしたものを微分対応させるためにはさらにVariable型にする必要がある\n",
    " - Variable使用時に自動微分を対応させるには独自関数を使用しないといけない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6011, 0.1274, 0.2992],\n",
      "        [0.5663, 0.7715, 0.6420],\n",
      "        [0.4517, 0.9517, 0.3666],\n",
      "        [0.3528, 0.7068, 0.5273],\n",
      "        [0.1872, 0.3867, 0.9556]])\n",
      "tensor([[0.6011, 0.1274, 0.2992],\n",
      "        [0.5663, 0.7715, 0.6420],\n",
      "        [0.4517, 0.9517, 0.3666],\n",
      "        [0.3528, 0.7068, 0.5273],\n",
      "        [0.1872, 0.3867, 0.9556]])\n",
      "tensor([[2.3614, 2.0162, 2.0895],\n",
      "        [2.3207, 2.5952, 2.4121],\n",
      "        [2.2040, 2.9056, 2.1344],\n",
      "        [2.1245, 2.4996, 2.2780],\n",
      "        [2.0350, 2.1495, 2.9132]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = Variable(x)\n",
    "z = torch.pow(y,2) + 2 #y_i**2 + 2\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape\n",
    "- numpyでも形状は非常に大事でした. それはtensorになっても変わりません.\n",
    "- pytorchではshapeでもsizeでも同じ結果になります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[ 0.1804,  0.5438,  0.2560],\n",
    "        [-0.9047, -0.3714, -0.7198],\n",
    "        [-1.8642, -0.3904,  1.0680],\n",
    "        [ 0.4988,  0.7488, -1.1682],\n",
    "        [ 0.3947,  0.5058,  1.7056]])\n",
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本はnumpy likeなのでスライシングも同じようにできます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  3,  6,  9, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.arange(15).reshape(5,3))\n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-hotベクトルにしたり, ベクトルを1行の行列にしたり, という作業はよくやりますね. このあたりに関しては, .view(,) が使えます.\n",
    "\n",
    "参照：[【PyTorch】Tensorを操作する関数（transpose、view、reshape）](https://qiita.com/kenta1984/items/d68b72214ce92beebbe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]])\n"
     ]
    }
   ],
   "source": [
    "xview = x.view(-1,15)\n",
    "print(xview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "- cudaを使って計算する場合にやることが簡単です.\n",
    "- tensorを作成するときに, device=で指定するか, .to(device)にするだけです.\n",
    "\n",
    "- .to()するときは、 x = x.to() しないと反映されないことに注意！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda <class 'torch.device'>\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]], device='cuda:0')\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print(device, type(device))\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.is_cuda)\n",
    "    print(z.to(\"cpu\", torch.double).is_cuda)       # ``.to`` can also change dtype together!\n",
    "    print(z.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの取得\n",
    "- pytorchで与えるデータはTensor(train), Tensor(target)のようにする必要がある。\n",
    "- データラベルを同時に変換させる関数としてTensorDatasetがある。\n",
    "- pytorchのDataLoaderはバッチ処理のみ対応している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = x\n",
    "#X_test = x\n",
    "#train = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "#train_loader = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True)\n",
    "#test = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "#test_loader = torch.utils.data.DataLoader(test, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "ニューラルネットワークの基本はBP(逆誤差伝播)です.\n",
    "すなわち, 微分操作が必要不可欠です.\n",
    "この章ではその微分操作を自動に行ってくれるAUTOGRADについて見ていきましょう."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- torch.Tensorには, .require_grad, .gradというattributeが, .backward()というメソッドを持っています.\n",
    "- この.require_gradをTrueに設定すれば, このtensorに与えられる演算を全て記憶し, .backward()により, 勾配を計算してくれます. その勾配の計算結果は, .gradが保持しています.\n",
    "- 計算グラフを構築するということでしょうか.\n",
    "- モデルのテスト時などの勾配計算が必要ないときは, .detach()メソッドにより解除できます.\n",
    "- これは, with torch.no_grad()のブロック文のなかで記述しても可能です. メモリの節約にもなります.\n",
    "- .grad_fnは, 勾配計算のために, 演算に用いた関数を示してくれます.\n",
    "\n",
    "と書いてみましたが, やってみないとよくわかりませんね.\n",
    "やってみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "True\n",
      "<AddBackward0 object at 0x7faaa9e95610>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)# 入力xのrequires_gradをTrueにしたので, yもTrueになっている\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y*y*3\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorialの例は少し煩雑なので, 簡単な例で見てみましょうか.\n",
    "\n",
    "y = e<sup>x</sup>\n",
    "\n",
    "dy/dx =  e<sup>x</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(7.3891, grad_fn=<ExpBackward>)\n",
      "tensor(7.3891)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(x)\n",
    "y = torch.exp(x)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微分操作をしない場合は, with torch.no_grad()を使いましょう.\n",
    "- 推論時には不要でメモリの無駄遣いになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "ニューラルネットワークの一般的なトレーニング手順は次のとおりです。\n",
    "\n",
    "- いくつかの学習可能なパラメーター（または重み）を持つニューラルネットワークを定義する\n",
    "- 入力のデータセットを反復処理します\n",
    "- ネットワークを介して入力を処理する\n",
    "- 損失を計算します（出力がどれほど正しいか）\n",
    "- 勾配をネットワークのパラメーターに反映\n",
    "- ネットワークの重みを更新します。通常は、単純な更新ルールを使用します。\n",
    "\n",
    "簡潔ですね. これ通り順に作って見ましょう.\n",
    "なお, この方法で作成したobjectは特別なことをしなくても, autogradに依存した形で記述されいるため,先のような手続きを踏むことなく自動微分が出来ます.\n",
    "\n",
    "#### Define the network\n",
    "\n",
    "基本的に, nn.Moduleというクラスを継承する形で作成していきます.  \n",
    "まずは, __init__()でLayerをメンバ変数に格納していきます.  \n",
    "よく使うLayerとしては, \n",
    "- Affine変換の nn.Linear(in_features, out_features, bias=True), \n",
    "- 畳み込みのConv1d(in_channels, out_channels, kernel_size), Conv2d(in_channels, out_channels, kernel_size), \n",
    "- PooingのMaxPool1d(kernel_size), MaxPool2d(kernel_size), \n",
    "- 活性化関数のReLU(), \n",
    "- DropoutのDropout(), Dropout2d などでしょうか.\n",
    "\n",
    "詳しくは, torch.nnの公式documentを参考にLayerを構成していきましょう.\n",
    "\n",
    "その後は, forward()で順伝播の計算グラフを作成します. この辺りも見たまんまの感じです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 30, 30]              60\n",
      "            Conv2d-2           [-1, 16, 13, 13]             880\n",
      "            Linear-3                  [-1, 120]          69,240\n",
      "            Linear-4                   [-1, 84]          10,164\n",
      "            Linear-5                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.31\n",
      "Estimated Total Size (MB): 0.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "net.to(\"cuda:0\")\n",
    "summary(net, input_size=(1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータに関しては, .parameters()で取り出すことができます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "それではデータを入力してみましょう.\n",
    "\n",
    "なぜここで4次元のtensorを入力したのでしょうか?  \n",
    "それはこの文面を見ればわかります.\n",
    "\n",
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\n",
    "\n",
    "つまり, torch.nnはbatch処理しか受け付けないということです.  \n",
    "なので, fakeとして1次元でいいのでbatchの形にしてね, と書いてあります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1293, -0.0203,  0.0446,  0.0295, -0.1021,  0.0275,  0.0100,  0.0721,\n",
      "         -0.0710,  0.0974]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "input = input.to(\"cuda\")\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "その後は, 勾配を初期化し, 逆誤差伝播します.  \n",
    ".backward()メソッドを呼び出すことで, その変数のgradに勾配が格納されます.\n",
    "\n",
    "まだ損失関数をこの段階では定義していないので, これを計算しても(Errorは出ませんが), 返り値はNoneになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "#out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "\n",
    "それでは勾配を計算できるように損失関数を定義してみましょう.  \n",
    "pytorchでは, torch.nnかtorch.nn.functionalにある損失関数を使うのが一般的です."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2651, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10, device=\"cuda\")  # a dummy target, for example\n",
    "target = target.view(1, -1) #it thesame shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0074,  0.0064,  0.0017,  0.0170,  0.0131,  0.0078], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the weights\n",
    "勾配を計算しただけで満足してはNeural Networkは学習しません. updateしましょう.\n",
    "\n",
    "パラメータの更新方法には, SGD, Adam, Momentumなど複数ありますね.  \n",
    "pytorchでは, torch.optimを使って更新を行うobjectを生成し, optimizer.step()で更新を実行します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
