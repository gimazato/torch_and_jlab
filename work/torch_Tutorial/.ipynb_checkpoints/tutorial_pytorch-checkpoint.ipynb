{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Torch\n",
    "\n",
    "[ライトニングpytorch入門](https://qiita.com/sh-tatsuno/items/42fccff90c98103dffc9)\n",
    "\n",
    "[PyTorch入門 [公式Tutorial:DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZを読む]](https://qiita.com/north_redwing/items/30f9619f0ee727875250) -> 基本こっち主軸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #基本モジュール\n",
    "from torch.autograd import Variable #自動微分用\n",
    "import torch.nn as nn #ネットワーク構築用\n",
    "import torch.optim as optim #最適化関数\n",
    "import torch.nn.functional as F #ネットワーク用の様々な関数\n",
    "import torch.utils.data #データセット読み込み関連\n",
    "import torchvision #画像関連\n",
    "from torchvision import datasets, models, transforms #画像用データセット諸々"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "- pytorchはTensorという型で演算を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.4307e-43,  0.0000e+00,  9.4728e-43],\n",
      "        [ 0.0000e+00, -9.1675e+26,  3.0672e-41],\n",
      "        [-9.1675e+26,  3.0672e-41, -9.1675e+26],\n",
      "        [ 3.0672e-41, -9.0046e+26,  3.0672e-41],\n",
      "        [-9.1675e+26,  3.0672e-41, -9.1675e+26]])\n",
      "tensor([[0.7790, 0.3346, 0.0382],\n",
      "        [0.1074, 0.4717, 0.5779],\n",
      "        [0.8407, 0.2814, 0.9156],\n",
      "        [0.1337, 0.6923, 0.3997],\n",
      "        [0.0330, 0.4421, 0.0727]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor(5,3)# Construct a tensor directly from data:\n",
    "print(x)\n",
    "\n",
    "y=torch.rand(5,3)\n",
    "print(y)\n",
    "\n",
    "z = torch.zeros(5,3, dtype=torch.long)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "- pytorchを利用するには全ての変数はTensorに変換する必要がある\n",
    "\n",
    " - Tensor -> numpy ：(Tensorの変数).numpy()\n",
    "\n",
    " - numpy -> Tensor :torch.from_numpy(Numpyの変数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9071617  0.09814667 0.73439783]\n",
      " [0.07967415 0.09498776 0.4455739 ]\n",
      " [0.20109632 0.57980415 0.76843334]\n",
      " [0.42449622 0.49097182 0.19248178]\n",
      " [0.71854794 0.23940228 0.98574647]]\n",
      "tensor([[0.9072, 0.0981, 0.7344],\n",
      "        [0.0797, 0.0950, 0.4456],\n",
      "        [0.2011, 0.5798, 0.7684],\n",
      "        [0.4245, 0.4910, 0.1925],\n",
      "        [0.7185, 0.2394, 0.9857]], dtype=torch.float64)\n",
      "[[0.9071617  0.09814667 0.73439783]\n",
      " [0.07967415 0.09498776 0.4455739 ]\n",
      " [0.20109632 0.57980415 0.76843334]\n",
      " [0.42449622 0.49097182 0.19248178]\n",
      " [0.71854794 0.23940228 0.98574647]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(5, 3)\n",
    "y = torch.from_numpy(x)\n",
    "z = y.numpy()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参照渡し注意\n",
    "- ここで後半に注目しましょう. aから作成したbは, aを変更すると一緒に変更されいます. 参照渡しということです.\n",
    "- これ結構大事なことです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a= torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    "- Tensor型にしたものを微分対応させるためにはさらにVariable型にする必要がある\n",
    " - Variable使用時に自動微分を対応させるには独自関数を使用しないといけない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6000, 0.7893, 0.2879],\n",
      "        [0.7607, 0.8869, 0.9775],\n",
      "        [0.5582, 0.8618, 0.4328],\n",
      "        [0.5220, 0.6305, 0.6593],\n",
      "        [0.1771, 0.7225, 0.7839]])\n",
      "tensor([[0.6000, 0.7893, 0.2879],\n",
      "        [0.7607, 0.8869, 0.9775],\n",
      "        [0.5582, 0.8618, 0.4328],\n",
      "        [0.5220, 0.6305, 0.6593],\n",
      "        [0.1771, 0.7225, 0.7839]])\n",
      "tensor([[2.3600, 2.6230, 2.0829],\n",
      "        [2.5787, 2.7865, 2.9555],\n",
      "        [2.3116, 2.7427, 2.1873],\n",
      "        [2.2725, 2.3975, 2.4347],\n",
      "        [2.0313, 2.5221, 2.6145]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = Variable(x)\n",
    "z = torch.pow(y,2) + 2 #y_i**2 + 2\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape\n",
    "- numpyでも形状は非常に大事でした. それはtensorになっても変わりません.\n",
    "- pytorchではshapeでもsizeでも同じ結果になります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[ 0.1804,  0.5438,  0.2560],\n",
    "        [-0.9047, -0.3714, -0.7198],\n",
    "        [-1.8642, -0.3904,  1.0680],\n",
    "        [ 0.4988,  0.7488, -1.1682],\n",
    "        [ 0.3947,  0.5058,  1.7056]])\n",
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本はnumpy likeなのでスライシングも同じようにできます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  3,  6,  9, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.arange(15).reshape(5,3))\n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-hotベクトルにしたり, ベクトルを1行の行列にしたり, という作業はよくやりますね. このあたりに関しては, .view(,) が使えます.\n",
    "\n",
    "参照：[【PyTorch】Tensorを操作する関数（transpose、view、reshape）](https://qiita.com/kenta1984/items/d68b72214ce92beebbe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]])\n"
     ]
    }
   ],
   "source": [
    "xview = x.view(-1,15)\n",
    "print(xview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "- cudaを使って計算する場合にやることが簡単です.\n",
    "- tensorを作成するときに, device=で指定するか, .to(device)にするだけです.\n",
    "\n",
    "- .to()するときは、 x = x.to() しないと反映されないことに注意！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda <class 'torch.device'>\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]], device='cuda:0')\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print(device, type(device))\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.is_cuda)\n",
    "    print(z.to(\"cpu\", torch.double).is_cuda)       # ``.to`` can also change dtype together!\n",
    "    print(z.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの取得\n",
    "- pytorchで与えるデータはTensor(train), Tensor(target)のようにする必要がある。\n",
    "- データラベルを同時に変換させる関数としてTensorDatasetがある。\n",
    "- pytorchのDataLoaderはバッチ処理のみ対応している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = x\n",
    "#X_test = x\n",
    "#train = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "#train_loader = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True)\n",
    "#test = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "#test_loader = torch.utils.data.DataLoader(test, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "ニューラルネットワークの基本はBP(逆誤差伝播)です.\n",
    "すなわち, 微分操作が必要不可欠です.\n",
    "この章ではその微分操作を自動に行ってくれるAUTOGRADについて見ていきましょう."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- torch.Tensorには, .require_grad, .gradというattributeが, .backward()というメソッドを持っています.\n",
    "- この.require_gradをTrueに設定すれば, このtensorに与えられる演算を全て記憶し, .backward()により, 勾配を計算してくれます. その勾配の計算結果は, .gradが保持しています.\n",
    "- 計算グラフを構築するということでしょうか.\n",
    "- モデルのテスト時などの勾配計算が必要ないときは, .detach()メソッドにより解除できます.\n",
    "- これは, with torch.no_grad()のブロック文のなかで記述しても可能です. メモリの節約にもなります.\n",
    "- .grad_fnは, 勾配計算のために, 演算に用いた関数を示してくれます.\n",
    "\n",
    "と書いてみましたが, やってみないとよくわかりませんね.\n",
    "やってみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "True\n",
      "<AddBackward0 object at 0x7fd333379d50>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)# 入力xのrequires_gradをTrueにしたので, yもTrueになっている\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y*y*3\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorialの例は少し煩雑なので, 簡単な例で見てみましょうか.\n",
    "\n",
    "y = e<sup>x</sup>\n",
    "\n",
    "dy/dx =  e<sup>x</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(7.3891, grad_fn=<ExpBackward>)\n",
      "tensor(7.3891)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(x)\n",
    "y = torch.exp(x)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微分操作をしない場合は, with torch.no_grad()を使いましょう.\n",
    "- 推論時には不要でメモリの無駄遣いになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "ニューラルネットワークの一般的なトレーニング手順は次のとおりです。\n",
    "\n",
    "- いくつかの学習可能なパラメーター（または重み）を持つニューラルネットワークを定義する\n",
    "- 入力のデータセットを反復処理します\n",
    "- ネットワークを介して入力を処理する\n",
    "- 損失を計算します（出力がどれほど正しいか）\n",
    "- 勾配をネットワークのパラメーターに反映\n",
    "- ネットワークの重みを更新します。通常は、単純な更新ルールを使用します。\n",
    "\n",
    "簡潔ですね. これ通り順に作って見ましょう.\n",
    "なお, この方法で作成したobjectは特別なことをしなくても, autogradに依存した形で記述されいるため,先のような手続きを踏むことなく自動微分が出来ます.\n",
    "\n",
    "#### Define the network\n",
    "\n",
    "基本的に, nn.Moduleというクラスを継承する形で作成していきます.  \n",
    "まずは, __init__()でLayerをメンバ変数に格納していきます.  \n",
    "よく使うLayerとしては, \n",
    "- Affine変換の nn.Linear(in_features, out_features, bias=True), \n",
    "- 畳み込みのConv1d(in_channels, out_channels, kernel_size), Conv2d(in_channels, out_channels, kernel_size), \n",
    "- PooingのMaxPool1d(kernel_size), MaxPool2d(kernel_size), \n",
    "- 活性化関数のReLU(), \n",
    "- DropoutのDropout(), Dropout2d などでしょうか.\n",
    "\n",
    "詳しくは, torch.nnの公式documentを参考にLayerを構成していきましょう.\n",
    "\n",
    "その後は, forward()で順伝播の計算グラフを作成します. この辺りも見たまんまの感じです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
